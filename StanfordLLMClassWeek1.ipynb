{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmq5zEuvNt+yFqP/oGS/q1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imusicmash/stanford_llm_python/blob/main/StanfordLLMClassWeek1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLMs for business with Python class #1"
      ],
      "metadata": {
        "id": "RFP8rrV8AH4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if using an Nvidia GPU, you can check details about the compute with this command.\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6EZYAPM4SZF",
        "outputId": "07b22866-6dcf-4b79-e39e-d3ba2ee3c236"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 22 01:00:01 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the openai key from keys hidden in the colab secrets\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('openai')"
      ],
      "metadata": {
        "id": "0yzKH3Xy6L--"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJwrSkhwAURc",
        "outputId": "cada1765-ebb9-4e05-f766-7e4e713205f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/226.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m174.1/226.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "kWYDfsOOAVnc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "TMoD6cK7AtKd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documentation on using Chat API\n",
        "# https://platform.openai.com/docs/guides/text-generation/chat-completions-api\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":\"You are an AI that takes instructions from the human and produces a concise answer\"},\n",
        "        {\"role\":\"user\",\"content\":\"Compose a short haiku about students studying AI at Stanford late early in the morning\"}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "3_cFq2G6A8Hg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68b5NTi4B1oH",
        "outputId": "fbefed3e-5d60-4cfc-b090-b71eb818535d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Students dive deep in code,\n",
            "Stanford's dawn whispers wisdom,\n",
            "AI minds awaken.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an pirate that takes instructions from the human and produces a concise answer in rhyme\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the tallest building in NYC\"}\n",
        "    ]\n",
        ")\n",
        "#print(response)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9Mws_LKi5sJ",
        "outputId": "c2f4ab15-237c-45dd-accc-4d0fcc3fe212"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Empire State Building, me hearty,\n",
            "Stands tall and proud, reaching for the party.\n",
            "In New York City, it reigns supreme,\n",
            "A true skyscraper, like a pirate's dream.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3qrBwY0BYUf",
        "outputId": "96c756ce-d5d0-4d41-8e67-4acb7a7a8c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8tMjwkZjK5Zt9bXRxq7RikezSB1tf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The tallest building in NYC, you see,\\nIs One World Trade Center, a sight to behold, matey!', role='assistant', function_call=None, tool_calls=None))], created=1708206144, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_69829325d0', usage=CompletionUsage(completion_tokens=23, prompt_tokens=35, total_tokens=58))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].logprobs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiC05gftja3E",
        "outputId": "0718692c-cb89-4065-fa9b-91554ccb7474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].finish_reason)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ7d0wyljpIR",
        "outputId": "73066b98-fe62-418c-f9aa-04f5b463f9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and funny assistant\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you say something to inspire the audience of Neurips ai conference from December of 2023 held in New Orleans\"}\n",
        "    ]\n",
        ")\n",
        "#print(response)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVOfEyABcW7R",
        "outputId": "c29f7ae4-de1f-45af-f03d-48bc4eb8f6ab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure thing! \"Welcome to NeurIPS 2023 in New Orleans! Remember, in the world of AI, innovation knows no bounds. Let your creativity flow as freely as the Mississippi river and may your algorithms be as cutting-edge as the jazz here in the Big Easy. Let's make history together and remember, the only limit is your imagination!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message):\n",
        "    \"\"\"\n",
        "    Send a message to the OpenAI GPT-3.5 model and return its response.\n",
        "\n",
        "    This function interacts with the OpenAI API, specifically using the GPT-3.5-turbo model. It takes a user's message as input, sends it to the model, and returns the model's text-only response. The function ensures the AI's output is concise by providing a system-level instruction.\n",
        "\n",
        "    Parameters:\n",
        "    message (str): A string containing the user's message to the AI.\n",
        "\n",
        "    Returns:\n",
        "    str: The text response generated by the GPT-3.5 model.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        # response_format={ \"type\": \"json_object\" },\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an AI that takes instructions from a human and produces an answer. Be concise in your output.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{message}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    text_only = response.choices[0].message.content\n",
        "    return text_only"
      ],
      "metadata": {
        "id": "ctNwHOfjCvSH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"What is the capital of france)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zwriYpQvDRCH",
        "outputId": "9417be75-33f7-4b2b-c5de-da638117e099"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of France is Paris.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"What is the capital of Japan?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KJCDxw2Fj-j5",
        "outputId": "32b15afe-286d-4c1b-81b5-43c8102ca9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tokyo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vA_qaGR6EtCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kid_chat(message):\n",
        "    \"\"\"\n",
        "    Send a message to the OpenAI GPT-3.5 model and return its response.\n",
        "\n",
        "    This function interacts with the OpenAI API, specifically using the GPT-3.5-turbo model. It takes a user's message as input, sends it to the model, and returns the model's text-only response. The function ensures the AI's output is concise by providing a system-level instruction.\n",
        "\n",
        "    Parameters:\n",
        "    message (str): A string containing the user's message to the AI.\n",
        "\n",
        "    Returns:\n",
        "    str: The text response generated by the GPT-3.5 model.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        # response_format={ \"type\": \"json_object\" },\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a 2nd grade teacher that takes instructions from a child and produces an answer. Be concise in your output.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{message}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    text_only = response.choices[0].message.content\n",
        "    return text_only"
      ],
      "metadata": {
        "id": "qFTZ5JADEtV_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kid_chat(\"What is the theory of relativity?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "6qdIeor1FBC9",
        "outputId": "d2cc6a92-c25a-42af-e9a9-63d6b5e96450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The theory of relativity, created by Albert Einstein, explains how time and space are connected and how they are affected by gravity.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kid_chat(\"How does my computer work?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "h-L40vNqFJ_O",
        "outputId": "abed82ae-a0df-449e-deb0-a8363261a671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your computer works by using electricity to power its components, such as the processor, memory, and storage, which work together to run programs and display information on the screen.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kid_chat(\"How does my my car's automatic transmission work?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "lXwBFSI2VW0s",
        "outputId": "96fde1a4-9eeb-4683-bffb-e846539b774e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Your car's automatic transmission uses gears and a torque converter to automatically change the gear ratios as you drive, allowing the engine to run efficiently at various speeds without you having to shift gears manually.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gZDzdp3UFA3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdhmYc_oED66",
        "outputId": "a7c40b0a-e7ea-422d-83af-4a3e3777f639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.10.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# homework\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "open_ai_key = userdata.get('openai')\n",
        "\n",
        "client = OpenAI(api_key=open_ai_key)\n",
        "\n"
      ],
      "metadata": {
        "id": "zvyyGYKYUlUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Summarize the text into a headline\"},\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "\t      Meta will reward shareholders with its first-ever dividend and an additional $50bn in share buybacks\n",
        "        as bumper fourth-quarter results sent its shares up by more than 14 per cent in after-hours trading\n",
        "        on Thursday.\n",
        "        \"\"\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "summary = response.choices[0].message.content\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7tiJf5KD8Np",
        "outputId": "811847b3-9388-4dea-ea69-5afe9a77f825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta announces first-ever dividend and $50bn in share buybacks as fourth-quarter results drive shares up\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(message):\n",
        "  response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Summarize the text into 3 bullet points\"},\n",
        "    {\"role\": \"user\", \"content\":  f\"{message}\"}\n",
        "  ]\n",
        "  )\n",
        "  summary = response.choices[0].message.content\n",
        "  return summary"
      ],
      "metadata": {
        "id": "KPlEx_ziFEvX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize(\"\"\"Today, we are living through a time where technology of all kinds (AI, quantum, the metaverse, etc) is rapidly advancing, talent is being unlocked across the industry, and capital (for the most part), is going to businesses with real fundamentals and paths to profitability.\n",
        "\n",
        "So yes, I am optimistic and excited about the opportunities all around. And I believe you should be, too. But moreover, you should figure out how you can be a part of it.\n",
        "\n",
        "And I don’t mean to imply that there isn’t chaos everywhere you turn. There is. And amidst this constructive chaos, is opportunity.\n",
        "\n",
        "We’re seeing this in the number of new early stage companies being founded, in the U.S., and abroad. Many of these companies are being founded by experienced leaders, people who have lived through the ups and downs of other companies, and have the requisite experience and networks to make their ventures successful. We’re seeing this in the ongoing restructuring of large and medium sized tech companies, as they evolve their organization for the future, with some painful consequences for individual workers. And we’re seeing this in non-tech industries, with companies and the government focused on adapting to stay relevant and in charge.\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "ST7gBsQDEcCW",
        "outputId": "5b28071d-93d3-4671-f9ea-83ac4692cc31"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- Technology is rapidly advancing in areas such as AI, quantum, and the metaverse, unlocking talent across industries, and directing capital towards businesses with solid fundamentals and profitability paths.\\n- Despite the chaos in the current environment, there are ample opportunities arising, with many new early-stage companies being founded by experienced leaders and tech companies restructuring for the future.\\n- Companies, as well as governments, are focusing on adapting to stay relevant and maintain their positions, showcasing a trend of evolution and resilience in various industries beyond technology.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarize(\"\"\"After countless AI projects, one thing has become clear: the team must possess skills in research design, evaluation, NHST (Null Hypothesis Significance Testing), and other research-related areas to develop a viable and feasible solution. This is especially true for Generative AI projects. Throughout my career, including work at large tech companies such as Yahoo, eBay, and NVIDIA, I have worked with a great number of exceptional researchers and engineers. Whenever I enter a meeting with engineers, there is always a 100% confident answer for every problem, ready to be engineered. On the contrary, meetings with researchers often conclude with a plan for an experiment.\n",
        "\"\"\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "g7vgilewGVi1",
        "outputId": "63b5237f-ca5d-497c-ee4c-59314442a512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- To develop a successful AI project, teams must possess skills in research design, evaluation, NHST, and other research-related areas.\\n- Generative AI projects particularly require a strong emphasis on research and experimentation.\\n- Meetings with engineers tend to have 100% confident solutions ready to be implemented, while meetings with researchers often result in plans for further experimentation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(message):\n",
        "  response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Translate the content from Engish to Japanese\"},\n",
        "    {\"role\": \"user\", \"content\":  f\"{message}\"}\n",
        "  ]\n",
        "  )\n",
        "  summary = response.choices[0].message.content\n",
        "  return summary"
      ],
      "metadata": {
        "id": "XLETU3TaZw2A"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"\"\"Good morning. What would you like for breakfast. How about for dinner?\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1rKfkzqYfx-l",
        "outputId": "31fc6a04-bf91-4b6b-e31e-df035397c47c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'おはようございます。朝食は何がいいですか。夕食はどうですか？'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# about temperature Lower values for temperature result in more consistent outputs (e.g. 0.2), while higher values generate more diverse and creative results (e.g. 1.0)."
      ],
      "metadata": {
        "id": "dT2ME4BrWSTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genName(message):\n",
        "  response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with a product description and seed words, and your task is to generate product names.\"},\n",
        "    {\"role\": \"user\", \"content\":  f\"{message}\"}\n",
        "  ],\n",
        "  temperature=0.2\n",
        "   )\n",
        "  summary = response.choices[0].message.content\n",
        "  return summary"
      ],
      "metadata": {
        "id": "WL7JrTR5gEi9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genName(\"\"\"Product description: an all new MLOPs platform, that includes a GPU marketplace for low cost on-demand GPUS.\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E3PjJvtmP5K1",
        "outputId": "916033b8-e8d2-4c32-acd9-881fbe2fe4bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Product names:\\n1. GPUFlow\\n2. MLOpsXchange\\n3. GPUBoost\\n4. MLOpsMarket\\n5. GPUFlex\\n6. MLOpsGPU\\n7. GPUWave\\n8. MLOpsGrid\\n9. GPUPower\\n10. MLOpsHub'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genName(\"\"\"Product description: an drink delivery system that brings drinks to people who are out walking or jogging. Drinks are delivered with a drone. The runner only needs to hold out his cup.  He can be anywhere.\"\"\")"
      ],
      "metadata": {
        "id": "iCAe_0F7QQvY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "04e6531f-0fc5-45d3-9b53-30bbd39e7bf2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1. DroneQuench\\n2. AeroHydrate\\n3. SkySip\\n4. FlyThirst\\n5. AirRefresh\\n6. WingedHydration\\n7. DroneDrip\\n8. SoarSip\\n9. GlideGulp\\n10. AirQuench'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fn7ajJXkWqI5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}